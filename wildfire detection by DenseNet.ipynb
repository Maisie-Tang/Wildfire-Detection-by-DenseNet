{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dependencies**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from torch.utils import data as D\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyper parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 60\n",
    "batch_size = 64\n",
    "random_seed = 10\n",
    "initial_lr = 0.1\n",
    "# num_epoch = 250\n",
    "num_epoch = 5\n",
    "# num_images = 12620\n",
    "# num_images = 12620\n",
    "# split = round(num_images/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset split & Class speration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform_validation = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "root='/Users/maisie_tang/Documents/career/Picogrid/wildfire-smoke-detection-camera/train/', transform=transform_train)\n",
    "validset = torchvision.datasets.ImageFolder(\n",
    "root='/Users/maisie_tang/Documents/career/Picogrid/wildfire-smoke-detection-camera/train/', transform=transform_validation)\n",
    "testset = torchvision.datasets.ImageFolder(\n",
    "root='/Users/maisie_tang/Documents/career/Picogrid/wildfire-smoke-detection-camera/test/', transform=transform_test)\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = 450\n",
    "#train_idx_1 = indices[:1346]\n",
    "#train_idx_2 = indices[2693:]\n",
    "#train_idx = train_idx_1 + train_idx_2\n",
    "\n",
    "# train_idx = indices[split+1:]\n",
    "train_idx = indices[split+1:4301]\n",
    "valid_idx = indices[:split]\n",
    "\n",
    "num_test = len(testset)\n",
    "indices_test = list(range(num_test))\n",
    "test_idx = indices[:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=28, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "classes = ('wildfire','nonfire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DenseNet architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (dense_init): Conv2d(3, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (dense_init_2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (dense_block_1): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_1): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(72, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (dense_block_2): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(36, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transition_layer_2): Transition_layer(\n",
       "    (conv_1x1): bn_relu_conv(\n",
       "      (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(60, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (dense_block_3): DenseBlock(\n",
       "    (bottleneck_layer_0): bottleneck_layer(\n",
       "      (conv_1x1): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(30, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv_3x3): bn_relu_conv(\n",
       "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_layer): Linear(in_features=54, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class bn_relu_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
    "        super(bn_relu_conv, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(nin)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.batch_norm(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class bottleneck_layer(nn.Sequential):\n",
    "    def __init__(self, nin, growth_rate, drop_rate=0.2):    \n",
    "        super(bottleneck_layer, self).__init__()\n",
    "      \n",
    "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
    "        self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "      \n",
    "        self.drop_rate = drop_rate\n",
    "      \n",
    "    def forward(self, x):\n",
    "        bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
    "          \n",
    "        bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
    "      \n",
    "        return bottleneck_output\n",
    "\n",
    "class Transition_layer(nn.Sequential):\n",
    "    def __init__(self, nin, theta=0.5):    \n",
    "        super(Transition_layer, self).__init__()\n",
    "      \n",
    "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
    "        self.add_module('avg_pool_2x2', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
    "\n",
    "class DenseBlock(nn.Sequential):\n",
    "    def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "                        \n",
    "        for i in range(num_bottleneck_layers):\n",
    "            nin_bottleneck_layer = nin + growth_rate * i\n",
    "            self.add_module('bottleneck_layer_%d' % i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, num_layers=10, theta=0.5, drop_rate=0.2, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        assert (num_layers - 4) % 6 == 0\n",
    "\n",
    "        num_bottleneck_layers = (num_layers - 4) // 6\n",
    "\n",
    "        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=7, stride=2, padding=3, bias=True)\n",
    "        self.dense_init_2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
    "\n",
    "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
    "        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
    "\n",
    "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n",
    "\n",
    "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_init_output = self.dense_init(x)\n",
    "        dense_init_output_2 = self.dense_init_2(dense_init_output)\n",
    "\n",
    "        dense_block_1_output = self.dense_block_1(dense_init_output_2)\n",
    "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
    "\n",
    "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
    "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
    "\n",
    "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
    "\n",
    "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))                \n",
    "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
    "\n",
    "        output = self.fc_layer(global_avg_pool_output_flat)\n",
    "\n",
    "        return output\n",
    "    \n",
    "# def Optimized_DenseNet():\n",
    "#     return DenseNet(growth_rate=24, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=2)\n",
    "\n",
    "def Optimized_DenseNet():\n",
    "    return DenseNet(growth_rate=24, num_layers=10, theta=0.5, drop_rate=0.2, num_classes=2)\n",
    "\n",
    "net = Optimized_DenseNet()\n",
    "densenet = net.to(device)\n",
    "densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 48, 112, 112]           7,104\n",
      "         MaxPool2d-2           [-1, 48, 56, 56]               0\n",
      "       BatchNorm2d-3           [-1, 48, 56, 56]              96\n",
      "              ReLU-4           [-1, 48, 56, 56]               0\n",
      "            Conv2d-5           [-1, 96, 56, 56]           4,608\n",
      "      bn_relu_conv-6           [-1, 96, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 96, 56, 56]             192\n",
      "              ReLU-8           [-1, 96, 56, 56]               0\n",
      "            Conv2d-9           [-1, 24, 56, 56]          20,736\n",
      "     bn_relu_conv-10           [-1, 24, 56, 56]               0\n",
      "      BatchNorm2d-11           [-1, 72, 56, 56]             144\n",
      "             ReLU-12           [-1, 72, 56, 56]               0\n",
      "           Conv2d-13           [-1, 36, 56, 56]           2,592\n",
      "     bn_relu_conv-14           [-1, 36, 56, 56]               0\n",
      "        AvgPool2d-15           [-1, 36, 28, 28]               0\n",
      "      BatchNorm2d-16           [-1, 36, 28, 28]              72\n",
      "             ReLU-17           [-1, 36, 28, 28]               0\n",
      "           Conv2d-18           [-1, 96, 28, 28]           3,456\n",
      "     bn_relu_conv-19           [-1, 96, 28, 28]               0\n",
      "      BatchNorm2d-20           [-1, 96, 28, 28]             192\n",
      "             ReLU-21           [-1, 96, 28, 28]               0\n",
      "           Conv2d-22           [-1, 24, 28, 28]          20,736\n",
      "     bn_relu_conv-23           [-1, 24, 28, 28]               0\n",
      "      BatchNorm2d-24           [-1, 60, 28, 28]             120\n",
      "             ReLU-25           [-1, 60, 28, 28]               0\n",
      "           Conv2d-26           [-1, 30, 28, 28]           1,800\n",
      "     bn_relu_conv-27           [-1, 30, 28, 28]               0\n",
      "        AvgPool2d-28           [-1, 30, 14, 14]               0\n",
      "      BatchNorm2d-29           [-1, 30, 14, 14]              60\n",
      "             ReLU-30           [-1, 30, 14, 14]               0\n",
      "           Conv2d-31           [-1, 96, 14, 14]           2,880\n",
      "     bn_relu_conv-32           [-1, 96, 14, 14]               0\n",
      "      BatchNorm2d-33           [-1, 96, 14, 14]             192\n",
      "             ReLU-34           [-1, 96, 14, 14]               0\n",
      "           Conv2d-35           [-1, 24, 14, 14]          20,736\n",
      "     bn_relu_conv-36           [-1, 24, 14, 14]               0\n",
      "           Linear-37                    [-1, 2]             110\n",
      "================================================================\n",
      "Total params: 85,826\n",
      "Trainable params: 85,826\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 28.63\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 29.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(net, (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training & Save the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maisie_tang/opt/anaconda3/envs/pandas_playground/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(000) batch (000) loss: 0.68582 ( 2.5)\n",
      "epoch(000) batch (001) loss: 0.70410 ( 1.9)\n",
      "epoch(000) batch (002) loss: 0.64030 ( 2.0)\n",
      "epoch(000) batch (003) loss: 0.72755 ( 2.0)\n",
      "epoch(000) batch (004) loss: 0.65305 ( 2.0)\n",
      "epoch(000) batch (005) loss: 0.65920 ( 2.0)\n",
      "epoch(000) batch (006) loss: 0.56103 ( 1.9)\n",
      "epoch(000) batch (007) loss: 0.56078 ( 1.9)\n",
      "epoch(000) batch (008) loss: 0.61485 ( 2.0)\n",
      "epoch(000) batch (009) loss: 0.55693 ( 2.0)\n",
      "epoch(000) batch (010) loss: 0.58210 ( 2.0)\n",
      "epoch(000) batch (011) loss: 0.57934 ( 2.0)\n",
      "epoch(000) batch (012) loss: 0.73924 ( 2.2)\n",
      "epoch(000) batch (013) loss: 0.55141 ( 2.0)\n",
      "epoch(000) batch (014) loss: 0.56469 ( 2.0)\n",
      "epoch(000) batch (015) loss: 0.52456 ( 2.0)\n",
      "epoch(000) batch (016) loss: 0.39980 ( 2.1)\n",
      "epoch(000) batch (017) loss: 0.53175 ( 3.1)\n",
      "epoch(000) batch (018) loss: 0.49647 ( 3.7)\n",
      "epoch(000) batch (019) loss: 0.49401 ( 3.7)\n",
      "epoch(000) batch (020) loss: 0.60499 ( 2.6)\n",
      "epoch(000) batch (021) loss: 0.47321 ( 2.9)\n",
      "epoch(000) batch (022) loss: 0.50666 ( 2.1)\n",
      "epoch(000) batch (023) loss: 0.48296 ( 2.1)\n",
      "epoch(000) batch (024) loss: 0.48067 ( 2.1)\n",
      "epoch(000) batch (025) loss: 0.47672 ( 2.2)\n",
      "epoch(000) batch (026) loss: 0.54529 ( 2.1)\n",
      "epoch(000) batch (027) loss: 0.43299 ( 2.1)\n",
      "epoch(000) batch (028) loss: 0.50794 ( 2.4)\n",
      "epoch(000) batch (029) loss: 0.57781 ( 2.2)\n",
      "epoch(000) batch (030) loss: 0.35689 ( 2.2)\n",
      "epoch(000) batch (031) loss: 0.45240 ( 2.6)\n",
      "epoch(000) batch (032) loss: 0.50922 ( 5.8)\n",
      "epoch(000) batch (033) loss: 0.70084 ( 3.9)\n",
      "epoch(000) batch (034) loss: 0.51189 ( 2.4)\n",
      "epoch(000) batch (035) loss: 0.54733 ( 2.1)\n",
      "epoch(000) batch (036) loss: 0.51019 ( 2.1)\n",
      "epoch(000) batch (037) loss: 0.59796 ( 2.1)\n",
      "epoch(000) batch (038) loss: 0.58739 ( 2.1)\n",
      "epoch(000) batch (039) loss: 0.45790 ( 2.0)\n",
      "epoch(000) batch (040) loss: 0.34644 ( 2.1)\n",
      "epoch(000) batch (041) loss: 0.37365 ( 2.1)\n",
      "epoch(000) batch (042) loss: 0.36454 ( 2.0)\n",
      "epoch(000) batch (043) loss: 0.45206 ( 1.9)\n",
      "epoch(000) batch (044) loss: 0.48172 ( 2.0)\n",
      "epoch(000) batch (045) loss: 0.51347 ( 2.1)\n",
      "epoch(000) batch (046) loss: 0.46964 ( 2.0)\n",
      "epoch(000) batch (047) loss: 0.45846 ( 1.9)\n",
      "epoch(000) batch (048) loss: 0.38999 ( 1.9)\n",
      "epoch(000) batch (049) loss: 0.35703 ( 1.9)\n",
      "epoch(000) batch (050) loss: 0.28180 ( 1.9)\n",
      "epoch(000) batch (051) loss: 0.28163 ( 1.9)\n",
      "epoch(000) batch (052) loss: 0.47832 ( 2.0)\n",
      "epoch(000) batch (053) loss: 0.38443 ( 1.9)\n",
      "epoch(000) batch (054) loss: 0.32202 ( 1.9)\n",
      "epoch(000) batch (055) loss: 0.56713 ( 1.9)\n",
      "epoch(000) batch (056) loss: 0.34244 ( 1.9)\n",
      "epoch(000) batch (057) loss: 0.47646 ( 1.9)\n",
      "epoch(000) batch (058) loss: 0.50151 ( 1.9)\n",
      "epoch(000) batch (059) loss: 0.44238 ( 1.9)\n",
      "epoch(000) batch (060) loss: 0.62268 ( 0.3)\n",
      "epoch(001) batch (000) loss: 0.39352 ( 2.3)\n",
      "epoch(001) batch (001) loss: 0.50648 ( 1.9)\n",
      "epoch(001) batch (002) loss: 0.41865 ( 1.9)\n",
      "epoch(001) batch (003) loss: 0.39706 ( 1.9)\n",
      "epoch(001) batch (004) loss: 0.34318 ( 1.9)\n",
      "epoch(001) batch (005) loss: 0.46194 ( 2.0)\n",
      "epoch(001) batch (006) loss: 0.39142 ( 2.0)\n",
      "epoch(001) batch (007) loss: 0.42296 ( 1.9)\n",
      "epoch(001) batch (008) loss: 0.31390 ( 2.0)\n",
      "epoch(001) batch (009) loss: 0.48343 ( 2.0)\n",
      "epoch(001) batch (010) loss: 0.48410 ( 2.0)\n",
      "epoch(001) batch (011) loss: 0.33235 ( 1.9)\n",
      "epoch(001) batch (012) loss: 0.32715 ( 1.9)\n",
      "epoch(001) batch (013) loss: 0.43652 ( 1.9)\n",
      "epoch(001) batch (014) loss: 0.37860 ( 1.9)\n",
      "epoch(001) batch (015) loss: 0.45525 ( 2.0)\n",
      "epoch(001) batch (016) loss: 0.34340 ( 2.0)\n",
      "epoch(001) batch (017) loss: 0.34030 ( 2.0)\n",
      "epoch(001) batch (018) loss: 0.38988 ( 1.9)\n",
      "epoch(001) batch (019) loss: 0.35137 ( 1.9)\n",
      "epoch(001) batch (020) loss: 0.38931 ( 1.9)\n",
      "epoch(001) batch (021) loss: 0.43607 ( 2.0)\n",
      "epoch(001) batch (022) loss: 0.54409 ( 1.9)\n",
      "epoch(001) batch (023) loss: 0.27987 ( 1.9)\n",
      "epoch(001) batch (024) loss: 0.38799 ( 1.9)\n",
      "epoch(001) batch (025) loss: 0.26898 ( 1.9)\n",
      "epoch(001) batch (026) loss: 0.45737 ( 1.9)\n",
      "epoch(001) batch (027) loss: 0.37806 ( 1.9)\n",
      "epoch(001) batch (028) loss: 0.38455 ( 1.9)\n",
      "epoch(001) batch (029) loss: 0.32285 ( 1.9)\n",
      "epoch(001) batch (030) loss: 0.38372 ( 1.9)\n",
      "epoch(001) batch (031) loss: 0.43542 ( 1.9)\n",
      "epoch(001) batch (032) loss: 0.23293 ( 1.9)\n",
      "epoch(001) batch (033) loss: 0.39608 ( 1.9)\n",
      "epoch(001) batch (034) loss: 0.41595 ( 1.9)\n",
      "epoch(001) batch (035) loss: 0.35220 ( 1.9)\n",
      "epoch(001) batch (036) loss: 0.34280 ( 2.0)\n",
      "epoch(001) batch (037) loss: 0.43219 ( 2.0)\n",
      "epoch(001) batch (038) loss: 0.37560 ( 2.0)\n",
      "epoch(001) batch (039) loss: 0.34384 ( 1.9)\n",
      "epoch(001) batch (040) loss: 0.30509 ( 1.9)\n",
      "epoch(001) batch (041) loss: 0.30281 ( 2.0)\n",
      "epoch(001) batch (042) loss: 0.34677 ( 2.0)\n",
      "epoch(001) batch (043) loss: 0.40828 ( 1.9)\n",
      "epoch(001) batch (044) loss: 0.32267 ( 1.9)\n",
      "epoch(001) batch (045) loss: 0.36877 ( 1.9)\n",
      "epoch(001) batch (046) loss: 0.39327 ( 2.0)\n",
      "epoch(001) batch (047) loss: 0.29305 ( 1.9)\n",
      "epoch(001) batch (048) loss: 0.30797 ( 1.9)\n",
      "epoch(001) batch (049) loss: 0.36068 ( 1.9)\n",
      "epoch(001) batch (050) loss: 0.39760 ( 1.9)\n",
      "epoch(001) batch (051) loss: 0.26847 ( 1.9)\n",
      "epoch(001) batch (052) loss: 0.44688 ( 1.9)\n",
      "epoch(001) batch (053) loss: 0.21413 ( 1.9)\n",
      "epoch(001) batch (054) loss: 0.35133 ( 1.9)\n",
      "epoch(001) batch (055) loss: 0.42063 ( 1.9)\n",
      "epoch(001) batch (056) loss: 0.33634 ( 2.1)\n",
      "epoch(001) batch (057) loss: 0.39145 ( 2.1)\n",
      "epoch(001) batch (058) loss: 0.35366 ( 2.1)\n",
      "epoch(001) batch (059) loss: 0.27799 ( 2.0)\n",
      "epoch(001) batch (060) loss: 0.39471 ( 0.3)\n",
      "epoch(002) batch (000) loss: 0.32436 ( 2.5)\n",
      "epoch(002) batch (001) loss: 0.36565 ( 2.1)\n",
      "epoch(002) batch (002) loss: 0.39057 ( 2.1)\n",
      "epoch(002) batch (003) loss: 0.42680 ( 2.0)\n",
      "epoch(002) batch (004) loss: 0.41407 ( 2.1)\n",
      "epoch(002) batch (005) loss: 0.28697 ( 2.1)\n",
      "epoch(002) batch (006) loss: 0.43231 ( 2.1)\n",
      "epoch(002) batch (007) loss: 0.18716 ( 2.1)\n",
      "epoch(002) batch (008) loss: 0.26704 ( 2.1)\n",
      "epoch(002) batch (009) loss: 0.34904 ( 2.1)\n",
      "epoch(002) batch (010) loss: 0.48977 ( 2.1)\n",
      "epoch(002) batch (011) loss: 0.35852 ( 2.2)\n",
      "epoch(002) batch (012) loss: 0.31853 ( 2.1)\n",
      "epoch(002) batch (013) loss: 0.24781 ( 2.2)\n",
      "epoch(002) batch (014) loss: 0.44557 ( 2.1)\n",
      "epoch(002) batch (015) loss: 0.50160 ( 2.0)\n",
      "epoch(002) batch (016) loss: 0.37921 ( 2.0)\n",
      "epoch(002) batch (017) loss: 0.31919 ( 2.2)\n",
      "epoch(002) batch (018) loss: 0.33140 ( 2.0)\n",
      "epoch(002) batch (019) loss: 0.39570 ( 2.2)\n",
      "epoch(002) batch (020) loss: 0.40743 ( 2.1)\n",
      "epoch(002) batch (021) loss: 0.28019 ( 2.1)\n",
      "epoch(002) batch (022) loss: 0.43778 ( 2.1)\n",
      "epoch(002) batch (023) loss: 0.38896 ( 2.1)\n",
      "epoch(002) batch (024) loss: 0.22665 ( 2.1)\n",
      "epoch(002) batch (025) loss: 0.28409 ( 2.2)\n",
      "epoch(002) batch (026) loss: 0.41841 ( 2.1)\n",
      "epoch(002) batch (027) loss: 0.35750 ( 2.0)\n",
      "epoch(002) batch (028) loss: 0.31366 ( 2.1)\n",
      "epoch(002) batch (029) loss: 0.32465 ( 2.2)\n",
      "epoch(002) batch (030) loss: 0.26589 ( 2.1)\n",
      "epoch(002) batch (031) loss: 0.28790 ( 2.1)\n",
      "epoch(002) batch (032) loss: 0.38278 ( 2.1)\n",
      "epoch(002) batch (033) loss: 0.34989 ( 2.0)\n",
      "epoch(002) batch (034) loss: 0.30975 ( 2.0)\n",
      "epoch(002) batch (035) loss: 0.31714 ( 2.0)\n",
      "epoch(002) batch (036) loss: 0.26760 ( 2.0)\n",
      "epoch(002) batch (037) loss: 0.35735 ( 2.0)\n",
      "epoch(002) batch (038) loss: 0.29785 ( 2.1)\n",
      "epoch(002) batch (039) loss: 0.40621 ( 2.1)\n",
      "epoch(002) batch (040) loss: 0.33355 ( 2.0)\n",
      "epoch(002) batch (041) loss: 0.23887 ( 2.0)\n",
      "epoch(002) batch (042) loss: 0.27777 ( 2.0)\n",
      "epoch(002) batch (043) loss: 0.39064 ( 2.0)\n",
      "epoch(002) batch (044) loss: 0.35177 ( 2.0)\n",
      "epoch(002) batch (045) loss: 0.29566 ( 2.1)\n",
      "epoch(002) batch (046) loss: 0.36384 ( 2.1)\n",
      "epoch(002) batch (047) loss: 0.38412 ( 2.1)\n",
      "epoch(002) batch (048) loss: 0.41098 ( 2.3)\n",
      "epoch(002) batch (049) loss: 0.36760 ( 2.0)\n",
      "epoch(002) batch (050) loss: 0.36124 ( 2.0)\n",
      "epoch(002) batch (051) loss: 0.35428 ( 2.1)\n",
      "epoch(002) batch (052) loss: 0.37722 ( 2.0)\n",
      "epoch(002) batch (053) loss: 0.34980 ( 2.0)\n",
      "epoch(002) batch (054) loss: 0.27541 ( 2.0)\n",
      "epoch(002) batch (055) loss: 0.36711 ( 2.0)\n",
      "epoch(002) batch (056) loss: 0.25641 ( 2.0)\n",
      "epoch(002) batch (057) loss: 0.39539 ( 2.0)\n",
      "epoch(002) batch (058) loss: 0.43860 ( 2.1)\n",
      "epoch(002) batch (059) loss: 0.38101 ( 2.1)\n",
      "epoch(002) batch (060) loss: 0.55645 ( 0.4)\n",
      "epoch(003) batch (000) loss: 0.45076 ( 2.3)\n",
      "epoch(003) batch (001) loss: 0.43935 ( 2.2)\n",
      "epoch(003) batch (002) loss: 0.31082 ( 2.0)\n",
      "epoch(003) batch (003) loss: 0.40191 ( 2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch(003) batch (004) loss: 0.33475 ( 2.0)\n",
      "epoch(003) batch (005) loss: 0.40843 ( 2.0)\n",
      "epoch(003) batch (006) loss: 0.34346 ( 2.0)\n",
      "epoch(003) batch (007) loss: 0.35319 ( 2.0)\n",
      "epoch(003) batch (008) loss: 0.31031 ( 2.0)\n",
      "epoch(003) batch (009) loss: 0.24246 ( 2.0)\n",
      "epoch(003) batch (010) loss: 0.36022 ( 2.0)\n",
      "epoch(003) batch (011) loss: 0.30893 ( 2.0)\n",
      "epoch(003) batch (012) loss: 0.29491 ( 2.0)\n",
      "epoch(003) batch (013) loss: 0.30347 ( 2.1)\n",
      "epoch(003) batch (014) loss: 0.46521 ( 2.1)\n",
      "epoch(003) batch (015) loss: 0.44617 ( 2.1)\n",
      "epoch(003) batch (016) loss: 0.31229 ( 2.2)\n",
      "epoch(003) batch (017) loss: 0.37680 ( 2.1)\n",
      "epoch(003) batch (018) loss: 0.34167 ( 2.1)\n",
      "epoch(003) batch (019) loss: 0.19208 ( 2.0)\n",
      "epoch(003) batch (020) loss: 0.32790 ( 2.3)\n",
      "epoch(003) batch (021) loss: 0.30819 ( 2.3)\n",
      "epoch(003) batch (022) loss: 0.28903 ( 2.1)\n",
      "epoch(003) batch (023) loss: 0.45504 ( 2.1)\n",
      "epoch(003) batch (024) loss: 0.29334 ( 2.1)\n",
      "epoch(003) batch (025) loss: 0.32501 ( 2.2)\n",
      "epoch(003) batch (026) loss: 0.26318 ( 2.2)\n",
      "epoch(003) batch (027) loss: 0.40070 ( 2.2)\n",
      "epoch(003) batch (028) loss: 0.40266 ( 2.2)\n",
      "epoch(003) batch (029) loss: 0.41517 ( 2.1)\n",
      "epoch(003) batch (030) loss: 0.24544 ( 2.2)\n",
      "epoch(003) batch (031) loss: 0.29173 ( 2.2)\n",
      "epoch(003) batch (032) loss: 0.35246 ( 2.1)\n",
      "epoch(003) batch (033) loss: 0.40825 ( 2.0)\n",
      "epoch(003) batch (034) loss: 0.53106 ( 2.1)\n",
      "epoch(003) batch (035) loss: 0.36238 ( 2.0)\n",
      "epoch(003) batch (036) loss: 0.25676 ( 2.1)\n",
      "epoch(003) batch (037) loss: 0.36403 ( 2.1)\n",
      "epoch(003) batch (038) loss: 0.40415 ( 2.1)\n",
      "epoch(003) batch (039) loss: 0.43703 ( 2.2)\n",
      "epoch(003) batch (040) loss: 0.33948 ( 2.2)\n",
      "epoch(003) batch (041) loss: 0.27090 ( 2.1)\n",
      "epoch(003) batch (042) loss: 0.38808 ( 2.1)\n",
      "epoch(003) batch (043) loss: 0.24901 ( 2.1)\n",
      "epoch(003) batch (044) loss: 0.35694 ( 2.1)\n",
      "epoch(003) batch (045) loss: 0.27385 ( 2.0)\n",
      "epoch(003) batch (046) loss: 0.22709 ( 2.1)\n",
      "epoch(003) batch (047) loss: 0.24824 ( 2.2)\n",
      "epoch(003) batch (048) loss: 0.28832 ( 2.1)\n",
      "epoch(003) batch (049) loss: 0.27480 ( 2.2)\n",
      "epoch(003) batch (050) loss: 0.33724 ( 2.1)\n",
      "epoch(003) batch (051) loss: 0.27727 ( 2.0)\n",
      "epoch(003) batch (052) loss: 0.34834 ( 2.0)\n",
      "epoch(003) batch (053) loss: 0.23449 ( 2.0)\n",
      "epoch(003) batch (054) loss: 0.28118 ( 2.0)\n",
      "epoch(003) batch (055) loss: 0.37275 ( 2.0)\n",
      "epoch(003) batch (056) loss: 0.36771 ( 1.9)\n",
      "epoch(003) batch (057) loss: 0.30031 ( 2.0)\n",
      "epoch(003) batch (058) loss: 0.36345 ( 2.0)\n",
      "epoch(003) batch (059) loss: 0.39613 ( 1.9)\n",
      "epoch(003) batch (060) loss: 0.58037 ( 0.3)\n",
      "epoch(004) batch (000) loss: 0.27552 ( 2.1)\n",
      "epoch(004) batch (001) loss: 0.35505 ( 1.9)\n",
      "epoch(004) batch (002) loss: 0.39570 ( 1.9)\n",
      "epoch(004) batch (003) loss: 0.44207 ( 1.9)\n",
      "epoch(004) batch (004) loss: 0.35771 ( 1.9)\n",
      "epoch(004) batch (005) loss: 0.29991 ( 1.9)\n",
      "epoch(004) batch (006) loss: 0.28640 ( 1.9)\n",
      "epoch(004) batch (007) loss: 0.32453 ( 1.9)\n",
      "epoch(004) batch (008) loss: 0.33550 ( 1.9)\n",
      "epoch(004) batch (009) loss: 0.29775 ( 1.9)\n",
      "epoch(004) batch (010) loss: 0.41165 ( 2.0)\n",
      "epoch(004) batch (011) loss: 0.36200 ( 1.9)\n",
      "epoch(004) batch (012) loss: 0.24584 ( 1.9)\n",
      "epoch(004) batch (013) loss: 0.36290 ( 1.9)\n",
      "epoch(004) batch (014) loss: 0.32916 ( 1.9)\n",
      "epoch(004) batch (015) loss: 0.32506 ( 1.9)\n",
      "epoch(004) batch (016) loss: 0.34264 ( 1.9)\n",
      "epoch(004) batch (017) loss: 0.24195 ( 1.9)\n",
      "epoch(004) batch (018) loss: 0.33791 ( 1.9)\n",
      "epoch(004) batch (019) loss: 0.38446 ( 1.9)\n",
      "epoch(004) batch (020) loss: 0.41571 ( 1.9)\n",
      "epoch(004) batch (021) loss: 0.29068 ( 1.9)\n",
      "epoch(004) batch (022) loss: 0.32641 ( 1.9)\n",
      "epoch(004) batch (023) loss: 0.33905 ( 1.9)\n",
      "epoch(004) batch (024) loss: 0.36458 ( 1.9)\n",
      "epoch(004) batch (025) loss: 0.33049 ( 2.0)\n",
      "epoch(004) batch (026) loss: 0.38519 ( 1.9)\n",
      "epoch(004) batch (027) loss: 0.25346 ( 1.9)\n",
      "epoch(004) batch (028) loss: 0.31472 ( 1.9)\n",
      "epoch(004) batch (029) loss: 0.23686 ( 1.9)\n",
      "epoch(004) batch (030) loss: 0.25564 ( 1.9)\n",
      "epoch(004) batch (031) loss: 0.36634 ( 1.9)\n",
      "epoch(004) batch (032) loss: 0.37475 ( 1.9)\n",
      "epoch(004) batch (033) loss: 0.35412 ( 1.9)\n",
      "epoch(004) batch (034) loss: 0.27028 ( 1.9)\n",
      "epoch(004) batch (035) loss: 0.23595 ( 1.9)\n",
      "epoch(004) batch (036) loss: 0.30211 ( 2.0)\n",
      "epoch(004) batch (037) loss: 0.34808 ( 2.0)\n",
      "epoch(004) batch (038) loss: 0.38999 ( 1.9)\n",
      "epoch(004) batch (039) loss: 0.39176 ( 1.9)\n",
      "epoch(004) batch (040) loss: 0.35001 ( 1.9)\n",
      "epoch(004) batch (041) loss: 0.30688 ( 2.0)\n",
      "epoch(004) batch (042) loss: 0.38325 ( 2.0)\n",
      "epoch(004) batch (043) loss: 0.34975 ( 2.0)\n",
      "epoch(004) batch (044) loss: 0.29614 ( 2.0)\n",
      "epoch(004) batch (045) loss: 0.35042 ( 3.7)\n",
      "epoch(004) batch (046) loss: 0.34332 ( 2.7)\n",
      "epoch(004) batch (047) loss: 0.35590 ( 2.2)\n",
      "epoch(004) batch (048) loss: 0.40483 ( 2.7)\n",
      "epoch(004) batch (049) loss: 0.37508 ( 2.2)\n",
      "epoch(004) batch (050) loss: 0.35711 ( 2.2)\n",
      "epoch(004) batch (051) loss: 0.37548 ( 2.2)\n",
      "epoch(004) batch (052) loss: 0.28219 ( 2.3)\n",
      "epoch(004) batch (053) loss: 0.32761 ( 2.2)\n",
      "epoch(004) batch (054) loss: 0.39650 ( 2.1)\n",
      "epoch(004) batch (055) loss: 0.33096 ( 2.1)\n",
      "epoch(004) batch (056) loss: 0.30778 ( 2.0)\n",
      "epoch(004) batch (057) loss: 0.40637 ( 2.0)\n",
      "epoch(004) batch (058) loss: 0.37163 ( 1.9)\n",
      "epoch(004) batch (059) loss: 0.39289 ( 2.0)\n",
      "epoch(004) batch (060) loss: 0.52438 ( 0.3)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "for epoch in range(num_epoch):  \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    epoch_loss, epoch_time = 0, 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        s_time = time.time()\n",
    "\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        show_period = 180\n",
    "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
    "           \n",
    "            running_loss = 0.0\n",
    "        \n",
    "        ### DEBUG\n",
    "        e_time = time.time()\n",
    "        batch_time = e_time - s_time\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'epoch({epoch:03d}) batch ({i:03d}) loss: {loss.item():0.5f} ({batch_time:4.1f})')\n",
    "        \n",
    "        \n",
    "    # validation part\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(valid_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "path = \"/Users/maisie_tang/Documents/career/Picogrid/savemodel/model2.pth\"\n",
    "torch.save(net.state_dict(),path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load model & test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8430, 0.1570],\n",
      "        [0.8200, 0.1800],\n",
      "        [0.8634, 0.1366],\n",
      "        [0.8034, 0.1966],\n",
      "        [0.8590, 0.1410],\n",
      "        [0.7853, 0.2147],\n",
      "        [0.8184, 0.1816],\n",
      "        [0.7154, 0.2846],\n",
      "        [0.9753, 0.0247],\n",
      "        [0.9935, 0.0065],\n",
      "        [0.9709, 0.0291],\n",
      "        [0.7141, 0.2859],\n",
      "        [0.9707, 0.0293],\n",
      "        [0.9880, 0.0120],\n",
      "        [0.9750, 0.0250],\n",
      "        [0.6182, 0.3818],\n",
      "        [0.9431, 0.0569],\n",
      "        [0.7669, 0.2331],\n",
      "        [0.9863, 0.0137],\n",
      "        [0.9766, 0.0234],\n",
      "        [0.9516, 0.0484],\n",
      "        [0.7184, 0.2816],\n",
      "        [0.9636, 0.0364],\n",
      "        [0.9769, 0.0231],\n",
      "        [0.9647, 0.0353],\n",
      "        [0.7620, 0.2380],\n",
      "        [0.9458, 0.0542],\n",
      "        [0.8992, 0.1008]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.8403, 0.1597],\n",
      "        [0.8765, 0.1235],\n",
      "        [0.8900, 0.1100],\n",
      "        [0.8861, 0.1139],\n",
      "        [0.8123, 0.1877],\n",
      "        [0.8041, 0.1959],\n",
      "        [0.8698, 0.1302],\n",
      "        [0.8757, 0.1243],\n",
      "        [0.8627, 0.1373],\n",
      "        [0.7701, 0.2299],\n",
      "        [0.9735, 0.0265],\n",
      "        [0.9417, 0.0583],\n",
      "        [0.7574, 0.2426],\n",
      "        [0.9184, 0.0816],\n",
      "        [0.9261, 0.0739],\n",
      "        [0.6265, 0.3735],\n",
      "        [0.8646, 0.1354],\n",
      "        [0.9276, 0.0724],\n",
      "        [0.9633, 0.0367],\n",
      "        [0.5566, 0.4434],\n",
      "        [0.9394, 0.0606],\n",
      "        [0.9889, 0.0111],\n",
      "        [0.9531, 0.0469],\n",
      "        [0.8347, 0.1653],\n",
      "        [0.9701, 0.0299],\n",
      "        [0.9541, 0.0459],\n",
      "        [0.9789, 0.0211],\n",
      "        [0.9414, 0.0586]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.8241, 0.1759],\n",
      "        [0.9645, 0.0355],\n",
      "        [0.9567, 0.0433],\n",
      "        [0.9259, 0.0741],\n",
      "        [0.9432, 0.0568],\n",
      "        [0.8086, 0.1914],\n",
      "        [0.9675, 0.0325],\n",
      "        [0.9154, 0.0846],\n",
      "        [0.9482, 0.0518],\n",
      "        [0.9410, 0.0590],\n",
      "        [0.9606, 0.0394],\n",
      "        [0.9494, 0.0506],\n",
      "        [0.9758, 0.0242],\n",
      "        [0.9560, 0.0440],\n",
      "        [0.9467, 0.0533],\n",
      "        [0.9428, 0.0572],\n",
      "        [0.9532, 0.0468],\n",
      "        [0.9364, 0.0636],\n",
      "        [0.7575, 0.2425],\n",
      "        [0.7503, 0.2497],\n",
      "        [0.9466, 0.0534],\n",
      "        [0.9545, 0.0455],\n",
      "        [0.8770, 0.1230],\n",
      "        [0.9584, 0.0416],\n",
      "        [0.9198, 0.0802],\n",
      "        [0.8429, 0.1572],\n",
      "        [0.8198, 0.1802],\n",
      "        [0.4073, 0.5927]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1])\n",
      "tensor([[0.7973, 0.2027],\n",
      "        [0.8615, 0.1385],\n",
      "        [0.7928, 0.2072],\n",
      "        [0.8545, 0.1455],\n",
      "        [0.6397, 0.3603],\n",
      "        [0.9745, 0.0255],\n",
      "        [0.9919, 0.0081],\n",
      "        [0.9880, 0.0120],\n",
      "        [0.6326, 0.3674],\n",
      "        [0.9736, 0.0264],\n",
      "        [0.7996, 0.2004],\n",
      "        [0.9880, 0.0120],\n",
      "        [0.9822, 0.0178],\n",
      "        [0.6905, 0.3095],\n",
      "        [0.9435, 0.0565],\n",
      "        [0.7935, 0.2065],\n",
      "        [0.9864, 0.0136],\n",
      "        [0.9838, 0.0162],\n",
      "        [0.9506, 0.0494],\n",
      "        [0.7443, 0.2557],\n",
      "        [0.9577, 0.0423],\n",
      "        [0.9873, 0.0127],\n",
      "        [0.9673, 0.0327],\n",
      "        [0.8419, 0.1581],\n",
      "        [0.9378, 0.0622],\n",
      "        [0.8980, 0.1020],\n",
      "        [0.8238, 0.1762],\n",
      "        [0.8873, 0.1127]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.8906, 0.1094],\n",
      "        [0.8862, 0.1138],\n",
      "        [0.8065, 0.1935],\n",
      "        [0.8056, 0.1944],\n",
      "        [0.9618, 0.0382],\n",
      "        [0.8823, 0.1177],\n",
      "        [0.8602, 0.1398],\n",
      "        [0.7653, 0.2347],\n",
      "        [0.9819, 0.0181],\n",
      "        [0.9339, 0.0661],\n",
      "        [0.8465, 0.1535],\n",
      "        [0.9554, 0.0446],\n",
      "        [0.9218, 0.0782],\n",
      "        [0.6056, 0.3944],\n",
      "        [0.8649, 0.1351],\n",
      "        [0.9542, 0.0458],\n",
      "        [0.7607, 0.2393],\n",
      "        [0.5923, 0.4077],\n",
      "        [0.9540, 0.0460],\n",
      "        [0.9882, 0.0118],\n",
      "        [0.7694, 0.2306],\n",
      "        [0.8383, 0.1617],\n",
      "        [0.9686, 0.0314],\n",
      "        [0.9537, 0.0463],\n",
      "        [0.9784, 0.0216],\n",
      "        [0.6158, 0.3842],\n",
      "        [0.8205, 0.1795],\n",
      "        [0.9655, 0.0345]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.9566, 0.0434],\n",
      "        [0.9294, 0.0706],\n",
      "        [0.9668, 0.0332],\n",
      "        [0.9254, 0.0746],\n",
      "        [0.9535, 0.0465],\n",
      "        [0.7951, 0.2049],\n",
      "        [0.9525, 0.0475],\n",
      "        [0.7767, 0.2233],\n",
      "        [0.9740, 0.0260],\n",
      "        [0.8192, 0.1808],\n",
      "        [0.9454, 0.0546],\n",
      "        [0.9339, 0.0661],\n",
      "        [0.8304, 0.1696],\n",
      "        [0.9287, 0.0713],\n",
      "        [0.7546, 0.2454],\n",
      "        [0.7570, 0.2430],\n",
      "        [0.9474, 0.0526],\n",
      "        [0.9558, 0.0442],\n",
      "        [0.3255, 0.6745],\n",
      "        [0.9565, 0.0435],\n",
      "        [0.7829, 0.2171],\n",
      "        [0.4467, 0.5533],\n",
      "        [0.8484, 0.1516],\n",
      "        [0.8146, 0.1854],\n",
      "        [0.8614, 0.1386],\n",
      "        [0.7997, 0.2003],\n",
      "        [0.8610, 0.1390],\n",
      "        [0.7895, 0.2105]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.9625, 0.0375],\n",
      "        [0.8580, 0.1420],\n",
      "        [0.7003, 0.2997],\n",
      "        [0.9685, 0.0315],\n",
      "        [0.9918, 0.0082],\n",
      "        [0.9734, 0.0266],\n",
      "        [0.6715, 0.3285],\n",
      "        [0.9725, 0.0275],\n",
      "        [0.8032, 0.1968],\n",
      "        [0.9881, 0.0119],\n",
      "        [0.9742, 0.0258],\n",
      "        [0.6581, 0.3419],\n",
      "        [0.9444, 0.0556],\n",
      "        [0.8111, 0.1889],\n",
      "        [0.9865, 0.0135],\n",
      "        [0.9768, 0.0232],\n",
      "        [0.9525, 0.0475],\n",
      "        [0.7056, 0.2944],\n",
      "        [0.9506, 0.0494],\n",
      "        [0.9758, 0.0242],\n",
      "        [0.8312, 0.1688],\n",
      "        [0.8068, 0.1932],\n",
      "        [0.9309, 0.0691],\n",
      "        [0.8966, 0.1034],\n",
      "        [0.8243, 0.1757],\n",
      "        [0.9223, 0.0777],\n",
      "        [0.8705, 0.1295],\n",
      "        [0.8878, 0.1122]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.7976, 0.2024],\n",
      "        [0.8208, 0.1792],\n",
      "        [0.8636, 0.1364],\n",
      "        [0.9825, 0.0175],\n",
      "        [0.8618, 0.1382],\n",
      "        [0.7601, 0.2399],\n",
      "        [0.9744, 0.0256],\n",
      "        [0.9302, 0.0698],\n",
      "        [0.8475, 0.1525],\n",
      "        [0.9198, 0.0802],\n",
      "        [0.9147, 0.0853],\n",
      "        [0.6233, 0.3767],\n",
      "        [0.8658, 0.1342],\n",
      "        [0.9300, 0.0700],\n",
      "        [0.9583, 0.0417],\n",
      "        [0.5447, 0.4553],\n",
      "        [0.9426, 0.0574],\n",
      "        [0.9881, 0.0119],\n",
      "        [0.9504, 0.0496],\n",
      "        [0.8360, 0.1640],\n",
      "        [0.9670, 0.0330],\n",
      "        [0.9564, 0.0436],\n",
      "        [0.9761, 0.0239],\n",
      "        [0.9490, 0.0510],\n",
      "        [0.8206, 0.1794],\n",
      "        [0.9659, 0.0341],\n",
      "        [0.9574, 0.0426],\n",
      "        [0.9316, 0.0684]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "tensor([[0.9431, 0.0569],\n",
      "        [0.8028, 0.1972],\n",
      "        [0.9274, 0.0726],\n",
      "        [0.9635, 0.0365],\n",
      "        [0.9420, 0.0580],\n",
      "        [0.9557, 0.0443],\n",
      "        [0.9479, 0.0521],\n",
      "        [0.9723, 0.0277],\n",
      "        [0.9560, 0.0440],\n",
      "        [0.9449, 0.0551],\n",
      "        [0.9215, 0.0785],\n",
      "        [0.9524, 0.0476],\n",
      "        [0.9356, 0.0644],\n",
      "        [0.7571, 0.2429],\n",
      "        [0.7546, 0.2454],\n",
      "        [0.9732, 0.0268],\n",
      "        [0.9903, 0.0097],\n",
      "        [0.9612, 0.0388],\n",
      "        [0.8779, 0.1221],\n",
      "        [0.9557, 0.0443],\n",
      "        [0.7985, 0.2015],\n",
      "        [0.9345, 0.0655],\n",
      "        [0.8435, 0.1565],\n",
      "        [0.8146, 0.1854],\n",
      "        [0.4286, 0.5714],\n",
      "        [0.7958, 0.2042],\n",
      "        [0.8588, 0.1412],\n",
      "        [0.7885, 0.2115]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0])\n",
      "tensor([[0.9637, 0.0363],\n",
      "        [0.7214, 0.2786],\n",
      "        [0.6442, 0.3558],\n",
      "        [0.7927, 0.2073],\n",
      "        [0.0077, 0.9923],\n",
      "        [0.0093, 0.9907],\n",
      "        [0.8678, 0.1322],\n",
      "        [0.0111, 0.9889],\n",
      "        [0.0071, 0.9929],\n",
      "        [0.8535, 0.1465],\n",
      "        [0.0354, 0.9646],\n",
      "        [0.0141, 0.9859],\n",
      "        [0.8258, 0.1742],\n",
      "        [0.0072, 0.9928],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.3644, 0.6356],\n",
      "        [0.0405, 0.9595],\n",
      "        [0.0250, 0.9750],\n",
      "        [0.4529, 0.5471],\n",
      "        [0.1248, 0.8752],\n",
      "        [0.1287, 0.8713],\n",
      "        [0.6829, 0.3171],\n",
      "        [0.0080, 0.9920],\n",
      "        [0.0149, 0.9851],\n",
      "        [0.7697, 0.2303],\n",
      "        [0.0076, 0.9924],\n",
      "        [0.0150, 0.9850],\n",
      "        [0.7980, 0.2020]])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0693, 0.9307],\n",
      "        [0.1534, 0.8466],\n",
      "        [0.8625, 0.1375],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.8381, 0.1619],\n",
      "        [0.0059, 0.9941],\n",
      "        [0.0101, 0.9899],\n",
      "        [0.7917, 0.2083],\n",
      "        [0.0022, 0.9978],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.9251, 0.0749],\n",
      "        [0.0250, 0.9750],\n",
      "        [0.0470, 0.9530],\n",
      "        [0.7799, 0.2201],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.7731, 0.2269],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.8927, 0.1073],\n",
      "        [0.0367, 0.9633],\n",
      "        [0.0673, 0.9327],\n",
      "        [0.9421, 0.0579],\n",
      "        [0.0100, 0.9900],\n",
      "        [0.0067, 0.9933],\n",
      "        [0.8706, 0.1294],\n",
      "        [0.1966, 0.8034]])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1])\n",
      "tensor([[0.1735, 0.8265],\n",
      "        [0.8947, 0.1053],\n",
      "        [0.0062, 0.9938],\n",
      "        [0.0048, 0.9952],\n",
      "        [0.7548, 0.2452],\n",
      "        [0.1004, 0.8996],\n",
      "        [0.2243, 0.7757],\n",
      "        [0.9033, 0.0967],\n",
      "        [0.0681, 0.9319],\n",
      "        [0.0194, 0.9806],\n",
      "        [0.8334, 0.1666],\n",
      "        [0.0697, 0.9303],\n",
      "        [0.0452, 0.9548],\n",
      "        [0.7979, 0.2021],\n",
      "        [0.0075, 0.9925],\n",
      "        [0.0099, 0.9901],\n",
      "        [0.4322, 0.5678],\n",
      "        [0.2386, 0.7614],\n",
      "        [0.0667, 0.9333],\n",
      "        [0.8463, 0.1537],\n",
      "        [0.0357, 0.9643],\n",
      "        [0.0146, 0.9854],\n",
      "        [0.2709, 0.7291],\n",
      "        [0.0903, 0.9097],\n",
      "        [0.0316, 0.9684],\n",
      "        [0.3535, 0.6465],\n",
      "        [0.0544, 0.9456],\n",
      "        [0.0343, 0.9657]])\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "tensor([[0.7214, 0.2786],\n",
      "        [0.1099, 0.8901],\n",
      "        [0.0664, 0.9336],\n",
      "        [0.7320, 0.2680],\n",
      "        [0.0943, 0.9057],\n",
      "        [0.0649, 0.9351],\n",
      "        [0.8078, 0.1922],\n",
      "        [0.0738, 0.9262],\n",
      "        [0.1747, 0.8253],\n",
      "        [0.8577, 0.1423],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.0019, 0.9981],\n",
      "        [0.8334, 0.1666],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.0101, 0.9899],\n",
      "        [0.4827, 0.5173],\n",
      "        [0.0096, 0.9904],\n",
      "        [0.0016, 0.9984],\n",
      "        [0.8014, 0.1986],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.0031, 0.9969],\n",
      "        [0.7277, 0.2723],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0030, 0.9970],\n",
      "        [0.7882, 0.2118],\n",
      "        [0.0022, 0.9978],\n",
      "        [0.0023, 0.9977],\n",
      "        [0.9316, 0.0684]])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0])\n",
      "tensor([[0.0231, 0.9769],\n",
      "        [0.0427, 0.9573],\n",
      "        [0.7761, 0.2239],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.8915, 0.1085],\n",
      "        [0.0142, 0.9858],\n",
      "        [0.0180, 0.9820],\n",
      "        [0.8066, 0.1934],\n",
      "        [0.0014, 0.9986],\n",
      "        [0.0039, 0.9961],\n",
      "        [0.8595, 0.1405],\n",
      "        [0.0032, 0.9968],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.8127, 0.1873],\n",
      "        [0.0016, 0.9984],\n",
      "        [0.0049, 0.9951],\n",
      "        [0.7874, 0.2126],\n",
      "        [0.1491, 0.8509],\n",
      "        [0.2521, 0.7479],\n",
      "        [0.8835, 0.1165],\n",
      "        [0.1079, 0.8921],\n",
      "        [0.0353, 0.9647],\n",
      "        [0.7950, 0.2050],\n",
      "        [0.0072, 0.9928],\n",
      "        [0.0100, 0.9900],\n",
      "        [0.8692, 0.1308],\n",
      "        [0.0100, 0.9900]])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1])\n",
      "tensor([[0.0069, 0.9931],\n",
      "        [0.8152, 0.1848],\n",
      "        [0.0080, 0.9920],\n",
      "        [0.0057, 0.9943],\n",
      "        [0.2000, 0.8000],\n",
      "        [0.0441, 0.9559],\n",
      "        [0.0550, 0.9450],\n",
      "        [0.6847, 0.3153],\n",
      "        [0.0077, 0.9923],\n",
      "        [0.0147, 0.9853],\n",
      "        [0.7589, 0.2411],\n",
      "        [0.0077, 0.9923],\n",
      "        [0.0155, 0.9845],\n",
      "        [0.7797, 0.2203],\n",
      "        [0.0678, 0.9322],\n",
      "        [0.1347, 0.8653],\n",
      "        [0.8582, 0.1418],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.0019, 0.9981],\n",
      "        [0.8286, 0.1714],\n",
      "        [0.0074, 0.9926],\n",
      "        [0.0129, 0.9871],\n",
      "        [0.7870, 0.2130],\n",
      "        [0.0022, 0.9978],\n",
      "        [0.0024, 0.9976],\n",
      "        [0.9233, 0.0767],\n",
      "        [0.0260, 0.9740],\n",
      "        [0.0479, 0.9521]])\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1])\n",
      "tensor([[0.7780, 0.2220],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.7706, 0.2294],\n",
      "        [0.0021, 0.9979],\n",
      "        [0.0019, 0.9981],\n",
      "        [0.9150, 0.0850],\n",
      "        [0.0100, 0.9900],\n",
      "        [0.0139, 0.9861],\n",
      "        [0.9206, 0.0794],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0054, 0.9946],\n",
      "        [0.8319, 0.1681],\n",
      "        [0.1832, 0.8168],\n",
      "        [0.1691, 0.8309],\n",
      "        [0.9005, 0.0995],\n",
      "        [0.0063, 0.9937],\n",
      "        [0.0044, 0.9956],\n",
      "        [0.9083, 0.0917],\n",
      "        [0.1030, 0.8970],\n",
      "        [0.1019, 0.8981],\n",
      "        [0.7983, 0.2017],\n",
      "        [0.0073, 0.9927],\n",
      "        [0.0103, 0.9897],\n",
      "        [0.3726, 0.6274],\n",
      "        [0.1688, 0.8312],\n",
      "        [0.0517, 0.9483],\n",
      "        [0.2809, 0.7191]])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "tensor([[0.1062, 0.8938],\n",
      "        [0.0424, 0.9576],\n",
      "        [0.2580, 0.7420],\n",
      "        [0.0621, 0.9379],\n",
      "        [0.0855, 0.9145],\n",
      "        [0.7057, 0.2943],\n",
      "        [0.2883, 0.7117],\n",
      "        [0.2277, 0.7723],\n",
      "        [0.7183, 0.2817],\n",
      "        [0.0981, 0.9019],\n",
      "        [0.0626, 0.9374],\n",
      "        [0.8656, 0.1344],\n",
      "        [0.0025, 0.9975],\n",
      "        [0.0023, 0.9977],\n",
      "        [0.7955, 0.2045],\n",
      "        [0.0759, 0.9241],\n",
      "        [0.1352, 0.8648],\n",
      "        [0.8544, 0.1456],\n",
      "        [0.0020, 0.9980],\n",
      "        [0.0018, 0.9982],\n",
      "        [0.8033, 0.1967],\n",
      "        [0.0073, 0.9927],\n",
      "        [0.0127, 0.9873],\n",
      "        [0.7562, 0.2438],\n",
      "        [0.0070, 0.9930],\n",
      "        [0.0036, 0.9964],\n",
      "        [0.7885, 0.2115],\n",
      "        [0.0022, 0.9978]])\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1])\n",
      "tensor([[0.0024, 0.9976],\n",
      "        [0.9203, 0.0797],\n",
      "        [0.0271, 0.9729],\n",
      "        [0.0476, 0.9524]])\n",
      "tensor([1, 0, 1, 1])\n",
      "Accuracy of the network on the 1031 test images: 85.70143884892086 %\n",
      "Accuracy of wildfire : 98.43137254901961 %\n",
      "251.0\n",
      "255.0\n",
      "Accuracy of nonfire : 71.11111111111111 %\n",
      "160.0\n",
      "225.0\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/maisie_tang/Documents/career/Picogrid/savemodel/model2.pth\"\n",
    "net.load_state_dict(torch.load(path))\n",
    "net.eval()\n",
    "\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        print(torch.nn.functional.softmax(outputs, dim=1))\n",
    "        print(predicted)     \n",
    "        for i in range(labels.shape[0]):\n",
    "            \n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the 1031 test images: %5s %%' % (100 * correct / total))            \n",
    "            \n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %5s %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    print(class_correct[i])\n",
    "    print(class_total[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Class activation map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maisie_tang/opt/anaconda3/envs/pandas_playground/lib/python3.8/site-packages/torchvision/transforms/transforms.py:187: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requirs_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0cf453bbfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/maisie_tang/Documents/career/Picogrid/wildfire-smoke-detection-camera/test/wildfire/ck0k9etuqjxhh0848k6i1mw2f.jpeg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mimg_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_cmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f0cf453bbfd6>\u001b[0m in \u001b[0;36mto_var\u001b[0;34m(x, requires_grad, volatile)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequirs_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requirs_grad' is not defined"
     ]
    }
   ],
   "source": [
    "class CAM(nn.Module):\n",
    "    def __init__(self, model_to_convert, get_fc_layer=lambda m: m.fc_layer,score_fn=F.softmax, resize=True):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(model_to_convert.children())[:-1])\n",
    "        self.fc = get_fc_layer(model_to_convert)\n",
    "        self.conv  =  nn.Conv2d(self.fc.in_features, self.fc.out_features, kernel_size=1)\n",
    "        self.conv.weight = nn.Parameter(self.fc.weight.data.unsqueeze(-1).unsqueeze(-1))\n",
    "        self.conv.bias = self.fc.bias\n",
    "        self.score_fn = score_fn\n",
    "        self.resize = resize\n",
    "        self.eval()\n",
    "        \n",
    "    def forward(self, x, out_size=None):\n",
    "        batch_size, c, *size = x.size()\n",
    "        feat = self.backbone(x)\n",
    "        cmap = self.score_fn(self.conv(feat))\n",
    "        if self.resize:\n",
    "            if out_size is None:\n",
    "                out_size = size\n",
    "            cmap = F.upsample(cmap, size=out_size, mode='bicubic')\n",
    "        pooled = F.adaptive_avg_pool2d(feat,output_size=1)\n",
    "        flatten = pooled.view(batch_size, -1)\n",
    "        cls_score = self.score_fn(self.fc(flatten))\n",
    "        weighted_cmap =  (cmap*cls_score.unsqueeze(-1).unsqueeze(-1)).sum(dim=1)\n",
    "        return cmap, cls_score, weighted_cmap\n",
    "    \n",
    "path = \"/Users/maisie_tang/Documents/career/Picogrid/savemodel/model1.pth\"\n",
    "net.load_state_dict(torch.load(path))\n",
    "cam = CAM(net)\n",
    "#assert not cam.training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"use gpu\")\n",
    "    cam = cam.cuda()\n",
    "    def to_var(x, requires_grad=False, volatile=False):\n",
    "        return Variable(x.cuda(), requires_grad=requires_grad, volatile=volatile)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, volatile=False):\n",
    "        return Variable(x, requires_grad=requirs_grad, volatile=volatile)\n",
    "\n",
    "\n",
    "target_size = (224,224)\n",
    "\n",
    "normalize = transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                                 [0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([transforms.Scale(target_size),transforms.CenterCrop(target_size),\n",
    "                                transforms.ToTensor()])\n",
    "from torch.autograd import Variable\n",
    "\n",
    "img_path = \"/Users/maisie_tang/Documents/career/Picogrid/wildfire-smoke-detection-camera/test/wildfire/ck0k9etuqjxhh0848k6i1mw2f.jpeg\"\n",
    "img = Image.open(img_path)\n",
    "img_v = to_var(transform(img).unsqueeze(0),volatile=True)\n",
    "\n",
    "cmap, score, weighted_cmap = cam(img_v)\n",
    "print(cmap.size())\n",
    "print(score.size())\n",
    "print(weighted_cmap.size())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "background = np.array(img.resize(target_size))\n",
    "color_map = weighted_cmap.data.cpu().numpy()[0]\n",
    "color_map = cmap.data.cpu().numpy()[0,1]\n",
    "#print(color_map)\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "plt.imshow(background)\n",
    "plt.imshow(color_map,cmap ='jet',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
